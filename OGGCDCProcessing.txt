"""
Oracle GoldenGate CDC Processing Library - PySpark Version (Avro Schema)
Chuyên dụng để xử lý dữ liệu CDC từ OGG Avro format với PySpark DataFrame
"""

from typing import Dict, Any, List, Optional
from enum import Enum

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.types import StructType, StructField, StringType
from pyspark.sql.functions import (
    col, lit, when, to_timestamp, coalesce, 
    regexp_replace, current_timestamp, date_format, 
    concat, unix_timestamp
)

class OGGOperationType(Enum):
    """Enum cho các loại operation trong OGG"""
    INSERT = "I"
    UPDATE = "U" 
    DELETE = "D"
    INITIAL = "INIT"

class OGGCDCProcessor:
    """
    Processor chuyên dụng cho Oracle GoldenGate CDC data với PySpark
    Hỗ trợ xử lý Avro format từ OGG Kafka Handler với structured schema
    """
    
    def __init__(self, 
                 spark_session: SparkSession,
                 record_key_field: str = "CUSTOMER_ID",
                 partition_key_field: Optional[str] = "BATCH_ID"):
        """
        Args:
            spark_session: SparkSession instance
            record_key_field: Trường dùng làm record key (primary key)
            partition_key_field: Trường dùng để partition data
        """
        self.spark = spark_session
        self.record_key_field = record_key_field
        self.partition_key_field = partition_key_field
    
    def to_spark_dataframe(self, input_df: DataFrame) -> DataFrame:
        """
        Chuyển đổi DataFrame với Avro schema thành flattened DataFrame
        
        Args:
            input_df: DataFrame với schema đã structured từ Avro
                     Expected columns: table, op_type, op_ts, current_ts, pos, before, after
            
        Returns:
            PySpark DataFrame với data đã được flattened
        """
        if input_df.count() == 0:
            # Return empty DataFrame với basic schema
            schema = StructType([
                StructField("_ogg_table", StringType(), True),
                StructField("_ogg_operation", StringType(), True),
                StructField("_ogg_op_timestamp", StringType(), True),
                StructField("_ogg_current_timestamp", StringType(), True),
                StructField("_ogg_position", StringType(), True)
            ])
            return self.spark.createDataFrame([], schema)
        
        # Validate required columns
        required_cols = ['table', 'op_type', 'op_ts', 'current_ts', 'pos', 'before', 'after']
        missing_cols = [col for col in required_cols if col not in input_df.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}. Available columns: {input_df.columns}")
        
        # Get business field names từ before/after schema
        business_fields = self._extract_business_fields(input_df)
        
        # Start với metadata columns
        result_df = input_df.withColumn(
            "_ogg_table", 
            col("table")
        ).withColumn(
            "_ogg_operation",
            col("op_type")
        ).withColumn(
            "_ogg_op_timestamp",
            col("op_ts")
        ).withColumn(
            "_ogg_current_timestamp", 
            col("current_ts")
        ).withColumn(
            "_ogg_position",
            col("pos")
        )
        
        # Flatten business fields dựa trên operation type
        for field_name in business_fields:
            result_df = result_df.withColumn(
                field_name,
                self._get_final_field_value(field_name)
            )
        
        # Add _is_deleted flag for DELETE operations
        result_df = result_df.withColumn(
            "_is_deleted",
            when(col("_ogg_operation") == "D", lit(True)).otherwise(lit(False))
        )
        
        # Drop original structured columns
        result_df = result_df.drop("table", "op_type", "op_ts", "current_ts", "pos", "before", "after")
        
        # Optimize timestamp columns
        result_df = self._optimize_timestamp_columns(result_df)
        
        return result_df
    
    def _extract_business_fields(self, df: DataFrame) -> List[str]:
        """
        Extract business field names từ before/after struct schema
        
        Args:
            df: Input DataFrame với structured schema
            
        Returns:
            List of business field names
        """
        business_fields = set()
        
        # Get fields từ before struct
        if 'before' in df.columns:
            before_fields = df.schema['before'].dataType.fieldNames()
            business_fields.update(before_fields)
        
        # Get fields từ after struct  
        if 'after' in df.columns:
            after_fields = df.schema['after'].dataType.fieldNames()
            business_fields.update(after_fields)
        
        return sorted(list(business_fields))
    
    def _get_final_field_value(self, field_name: str):
        """
        Get final field value dựa trên operation type với Spark expressions
        
        Args:
            field_name: Tên field cần extract
            
        Returns:
            Spark Column expression cho final value
        """
        return when(col("op_type") == "I", 
                   # INSERT: use after
                   col(f"after.{field_name}")
               ).when(col("op_type") == "U",
                   # UPDATE: prefer after, fallback to before
                   coalesce(col(f"after.{field_name}"), col(f"before.{field_name}"))
               ).when(col("op_type") == "D",
                   # DELETE: use before
                   col(f"before.{field_name}")
               ).otherwise(
                   # INITIAL or unknown: prefer after, fallback to before
                   coalesce(col(f"after.{field_name}"), col(f"before.{field_name}"))
               )
    
    def _optimize_timestamp_columns(self, df: DataFrame) -> DataFrame:
        """
        Optimize timestamp columns
        
        Args:
            df: DataFrame to optimize
            
        Returns:
            DataFrame với optimized timestamp columns
        """
        optimized_df = df
        
        # Convert OGG timestamp strings to timestamp type
        timestamp_cols = ['_ogg_op_timestamp', '_ogg_current_timestamp']
        
        for col_name in timestamp_cols:
            if col_name in df.columns:
                try:
                    optimized_df = optimized_df.withColumn(
                        col_name,
                        to_timestamp(col(col_name))
                    )
                except:
                    # If conversion fails, keep as string
                    pass
        
        # Auto-detect và convert timestamp columns dựa trên naming patterns
        datetime_patterns = ['_DATE', '_TIME', '_TIMESTAMP', '_TS', 'CREATED_', 'UPDATED_', 'LAST_']
        
        for column_name in df.columns:
            if any(pattern in column_name.upper() for pattern in datetime_patterns):
                try:
                    optimized_df = optimized_df.withColumn(
                        column_name,
                        to_timestamp(col(column_name))
                    )
                except:
                    # If conversion fails, keep original type
                    pass
        
        return optimized_df
    
    def prepare_for_hudi(self, df: DataFrame) -> DataFrame:
        """
        Chuẩn bị PySpark DataFrame cho Apache Hudi
        
        Args:
            df: DataFrame từ to_spark_dataframe()
            
        Returns:
            DataFrame sẵn sàng cho Hudi upsert
        """
        if df.count() == 0:
            return df
        
        hudi_df = df
        
        # Thêm Hudi metadata columns
        hudi_df = hudi_df.withColumn(
            "_hoodie_record_key", 
            col(self.record_key_field).cast(StringType())
        )
        
        # Partition path
        if self.partition_key_field and self.partition_key_field in df.columns:
            hudi_df = hudi_df.withColumn(
                "_hoodie_partition_path",
                regexp_replace(
                    concat(lit(f"{self.partition_key_field.lower()}="), 
                           col(self.partition_key_field).cast(StringType())),
                    "[^a-zA-Z0-9=_-]", "_"
                )
            )
        else:
            hudi_df = hudi_df.withColumn("_hoodie_partition_path", lit("default"))
        
        # Commit time
        hudi_df = hudi_df.withColumn(
            "_hoodie_commit_time",
            regexp_replace(
                date_format(current_timestamp(), "yyyyMMddHHmmssSSS"),
                "[-: ]", ""
            )
        )
        
        # Precombine field (sử dụng OGG timestamp)
        if '_ogg_op_timestamp' in df.columns:
            hudi_df = hudi_df.withColumn(
                "_hoodie_commit_seqno",
                unix_timestamp(col('_ogg_op_timestamp'))
            )
        else:
            hudi_df = hudi_df.withColumn(
                "_hoodie_commit_seqno",
                unix_timestamp()
            )
        
        return hudi_df
    
    def prepare_for_delta(self, df: DataFrame) -> DataFrame:
        """
        Chuẩn bị PySpark DataFrame cho Delta Lake
        
        Args:
            df: DataFrame từ to_spark_dataframe()
            
        Returns:
            DataFrame sẵn sàng cho Delta merge
        """
        if df.count() == 0:
            return df
        
        delta_df = df
        
        # Thêm Delta metadata
        delta_df = delta_df.withColumn(
            "_delta_operation",
            when(col("_ogg_operation") == "I", lit("INSERT"))
            .when(col("_ogg_operation") == "U", lit("UPDATE"))
            .when(col("_ogg_operation") == "D", lit("DELETE"))
            .otherwise(lit("UPSERT"))
        )
        
        return delta_df
    
    def get_change_summary(self, df: DataFrame) -> Dict[str, Any]:
        """
        Get summary statistics của CDC changes
        
        Args:
            df: Processed DataFrame
            
        Returns:
            Dictionary với change statistics
        """
        if df.count() == 0:
            return {}
        
        # Count by operation type
        operation_counts = df.groupBy("_ogg_operation").count().collect()
        
        summary = {
            'total_records': df.count(),
            'operations': {},
            'tables': [],
            'time_range': {}
        }
        
        for row in operation_counts:
            summary['operations'][row['_ogg_operation']] = row['count']
        
        # Get unique tables
        tables = df.select("_ogg_table").distinct().collect()
        summary['tables'] = [row['_ogg_table'] for row in tables]
        
        # Get time range if timestamp columns exist
        if '_ogg_op_timestamp' in df.columns:
            time_stats = df.agg({
                '_ogg_op_timestamp': 'min',
                '_ogg_op_timestamp': 'max'
            }).collect()[0]
            
            summary['time_range'] = {
                'earliest_change': time_stats[0],
                'latest_change': time_stats[1]
            }
        
        return summary
    
    def filter_by_operation(self, df: DataFrame, operations: List[str]) -> DataFrame:
        """
        Filter DataFrame theo operation types
        
        Args:
            df: Processed DataFrame
            operations: List of operation types to include (I, U, D)
            
        Returns:
            Filtered DataFrame
        """
        return df.filter(col("_ogg_operation").isin(operations))
    
    def filter_by_table(self, df: DataFrame, table_names: List[str]) -> DataFrame:
        """
        Filter DataFrame theo table names
        
        Args:
            df: Processed DataFrame
            table_names: List of table names to include
            
        Returns:
            Filtered DataFrame
        """
        return df.filter(col("_ogg_table").isin(table_names))
    
    def get_latest_changes_per_record(self, df: DataFrame) -> DataFrame:
        """
        Get latest change per record key (deduplicate)
        
        Args:
            df: Processed DataFrame
            
        Returns:
            DataFrame với latest change per record
        """
        if self.record_key_field not in df.columns:
            raise ValueError(f"Record key field '{self.record_key_field}' not found in DataFrame")
        
        from pyspark.sql.window import Window
        from pyspark.sql.functions import row_number, desc
        
        # Create window partitioned by record key, ordered by timestamp desc
        window = Window.partitionBy(col(self.record_key_field)) \
                      .orderBy(desc("_ogg_op_timestamp"))
        
        return df.withColumn("row_num", row_number().over(window)) \
                .filter(col("row_num") == 1) \
                .drop("row_num")

# Utility functions
def create_hudi_upsert_config(table_name: str, 
                            record_key: str = "_hoodie_record_key",
                            partition_path: str = "_hoodie_partition_path",
                            precombine_field: str = "_hoodie_commit_seqno") -> Dict[str, str]:
    """
    Tạo Hudi configuration cho upsert với PySpark
    
    Args:
        table_name: Tên Hudi table
        record_key: Record key field
        partition_path: Partition path field
        precombine_field: Precombine field
        
    Returns:
        Hudi configuration dict
    """
    return {
        'hoodie.table.name': table_name,
        'hoodie.datasource.write.recordkey.field': record_key,
        'hoodie.datasource.write.partitionpath.field': partition_path,
        'hoodie.datasource.write.table.name': table_name,
        'hoodie.datasource.write.operation': 'upsert',
        'hoodie.datasource.write.precombine.field': precombine_field,
        'hoodie.upsert.shuffle.parallelism': '2',
        'hoodie.insert.shuffle.parallelism': '2',
        'hoodie.datasource.write.hive_style_partitioning': 'true',
        'hoodie.datasource.write.table.type': 'COPY_ON_WRITE'
    }

def create_delta_merge_condition(record_key_field: str) -> str:
    """
    Tạo merge condition cho Delta Lake
    
    Args:
        record_key_field: Primary key field name
        
    Returns:
        SQL merge condition string
    """
    return f"target.{record_key_field} = source.{record_key_field}"

def read_avro_files(spark: SparkSession, file_path: str) -> DataFrame:
    """
    Read Avro files từ OGG Kafka sink connector
    
    Args:
        spark: SparkSession
        file_path: Path to Avro files
        
    Returns:
        DataFrame với structured schema
    """
    return spark.read.format("avro").load(file_path)

def create_kafka_stream(spark: SparkSession, 
                       kafka_bootstrap_servers: str,
                       topic: str,
                       starting_offsets: str = "earliest") -> DataFrame:
    """
    Tạo streaming DataFrame từ Kafka với Avro format
    
    Args:
        spark: SparkSession
        kafka_bootstrap_servers: Kafka bootstrap servers
        topic: Kafka topic name
        starting_offsets: Starting offset position
        
    Returns:
        Streaming DataFrame
    """
    return spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
        .option("subscribe", topic) \
        .option("startingOffsets", starting_offsets) \
        .load()

# Example usage
if __name__ == "__main__":
    # Tạo Spark session với Avro support
    spark = SparkSession.builder \
        .appName("OGGCDCProcessor_Avro") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    # Sample structured data (giống như từ Avro files)
    from pyspark.sql.types import StructType, StructField, StringType
    
    avro_schema = StructType([
        StructField("table", StringType(), True),
        StructField("op_type", StringType(), True),
        StructField("op_ts", StringType(), True),
        StructField("current_ts", StringType(), True),
        StructField("pos", StringType(), True),
        StructField("before", StructType([
            StructField("CUSTOMER_ID", StringType(), True),
            StructField("ACCOUNT_NUMBER", StringType(), True),
            StructField("FIRST_NAME", StringType(), True),
            StructField("LAST_NAME", StringType(), True),
            StructField("MIDDLE_NAME", StringType(), True),
            StructField("EMAIL", StringType(), True),
            StructField("PHONE_NUMBER", StringType(), True)
        ]), True),
        StructField("after", StructType([
            StructField("CUSTOMER_ID", StringType(), True),
            StructField("ACCOUNT_NUMBER", StringType(), True),
            StructField("FIRST_NAME", StringType(), True),
            StructField("LAST_NAME", StringType(), True),
            StructField("MIDDLE_NAME", StringType(), True),
            StructField("EMAIL", StringType(), True),
            StructField("PHONE_NUMBER", StringType(), True)
        ]), True)
    ])
    
    # Sample data
    sample_data = [
        (
            "LOGMINER.CUSTOMER_TABLE",
            "U",
            "2025-08-27 13:53:01.035610",
            "2025-08-27 13:54:15.887000", 
            "00000000000000029346",
            ("CUST001", "ACC001", "John", "Doe", "M", "john.doe@email.com", "123-456-7890"),  # before
            ("CUST001", "ACC001", "John", "Doe", "Michael", "john.michael@email.com", "123-456-7890")  # after
        ),
        (
            "LOGMINER.CUSTOMER_TABLE",
            "I", 
            "2025-08-27 13:55:01.035610",
            "2025-08-27 13:55:15.887000",
            "00000000000000029347",
            None,  # before (INSERT has no before)
            ("CUST002", "ACC002", "Jane", "Smith", None, "jane.smith@email.com", "987-654-3210")  # after
        )
    ]
    
    # Tạo input DataFrame
    input_df = spark.createDataFrame(sample_data, avro_schema)
    
    print("=== INPUT DATAFRAME (Avro Schema) ===")
    print(f"Input row count: {input_df.count()}")
    input_df.printSchema()
    input_df.show(truncate=False)
    
    # Khởi tạo processor
    processor = OGGCDCProcessor(
        spark_session=spark,
        record_key_field="CUSTOMER_ID",
        partition_key_field="ACCOUNT_NUMBER"
    )
    
    # Convert to flattened DataFrame  
    flattened_df = processor.to_spark_dataframe(input_df)
    
    print("\n=== FLATTENED DATAFRAME ===")
    print(f"Row count: {flattened_df.count()}")
    print(f"Column count: {len(flattened_df.columns)}")
    
    flattened_df.printSchema()
    flattened_df.show(truncate=False)
    
    # Change summary
    summary = processor.get_change_summary(flattened_df)
    print(f"\n=== CHANGE SUMMARY ===")
    print(f"Total records: {summary['total_records']}")
    print(f"Operations: {summary['operations']}")
    print(f"Tables: {summary['tables']}")
    
    # Filter examples
    print(f"\n=== FILTER BY OPERATION (UPDATE only) ===")
    updates_only = processor.filter_by_operation(flattened_df, ["U"])
    updates_only.show(truncate=False)
    
    # Prepare for Hudi
    hudi_df = processor.prepare_for_hudi(flattened_df)
    print(f"\n=== HUDI-READY DATAFRAME ===")
    hudi_cols = [col for col in hudi_df.columns if col.startswith('_hoodie_')]
    print(f"Hudi columns: {hudi_cols}")
    hudi_df.select("CUSTOMER_ID", "_hoodie_record_key", "_hoodie_partition_path").show(truncate=False)
    
    # Example: Read from Avro files
    # input_df = read_avro_files(spark, "/path/to/ogg/avro/files")
    # result_df = processor.to_spark_dataframe(input_df)
    
    # Example: Write to Hudi
    # hudi_config = create_hudi_upsert_config("customer_table")
    # hudi_df.write.format("hudi").options(**hudi_config).mode("append").save("/path/to/hudi/table")
    
    spark.stop()