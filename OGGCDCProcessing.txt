"""
Oracle GoldenGate CDC Processing Library - PySpark Version
Chuyên dụng để xử lý dữ liệu CDC từ OGG format với PySpark DataFrame
"""

import json
from typing import Dict, Any, List, Optional, Union
from datetime import datetime
from enum import Enum

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.types import (
    StructType, StructField, StringType, LongType, IntegerType, 
    FloatType, DoubleType, BooleanType, TimestampType, DateType, 
    BinaryType, ArrayType, MapType, DecimalType
)
from pyspark.sql.functions import (
    col, lit, when, to_timestamp, to_date, cast, 
    from_json, regexp_replace, current_timestamp
)

class OGGOperationType(Enum):
    """Enum cho các loại operation trong OGG"""
    INSERT = "I"
    UPDATE = "U" 
    DELETE = "D"
    INITIAL = "INIT"

class OGGCDCProcessor:
    """
    Processor chuyên dụng cho Oracle GoldenGate CDC data với PySpark
    Hỗ trợ xử lý format JSON từ OGG Kafka Handler
    """
    
    def __init__(self, 
                 spark_session: SparkSession,
                 record_key_field: str = "CUSTOMER_ID",
                 partition_key_field: Optional[str] = "BATCH_ID"):
        """
        Args:
            spark_session: SparkSession instance
            record_key_field: Trường dùng làm record key (primary key)
            partition_key_field: Trường dùng để partition data
        """
        self.spark = spark_session
        self.record_key_field = record_key_field
        self.partition_key_field = partition_key_field
    
    def parse_ogg_record(self, ogg_json: Union[str, Dict]) -> Dict[str, Any]:
        """
        Parse single OGG CDC record từ JSON
        
        Args:
            ogg_json: JSON string hoặc dict từ OGG
            
        Returns:
            Parsed record với metadata
        """
        if isinstance(ogg_json, str):
            record = json.loads(ogg_json)
        else:
            record = ogg_json.copy()
        
        # Extract metadata
        metadata = {
            'table_name': self._extract_string_value(record.get('table', {})),
            'operation_type': self._extract_string_value(record.get('op_type', {})),
            'operation_timestamp': self._extract_string_value(record.get('op_ts', {})),
            'current_timestamp': self._extract_string_value(record.get('current_ts', {})),
            'position': self._extract_string_value(record.get('pos', {}))
        }
        
        # Extract before/after data
        before_data = self._extract_row_data(record.get('before', {}).get('row', {}))
        after_data = self._extract_row_data(record.get('after', {}).get('row', {}))
        
        return {
            'metadata': metadata,
            'before': before_data,
            'after': after_data,
            'final_record': self._create_final_record(
                metadata['operation_type'], 
                before_data, 
                after_data
            )
        }
    
    def _extract_string_value(self, value_dict: Dict) -> str:
        """Extract giá trị string từ OGG nested structure"""
        if not value_dict:
            return ""
        return value_dict.get('string', '')
    
    def _extract_row_data(self, row_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract dữ liệu từ OGG row format với type mapping cho PySpark
        Format: {"FIELD_NAME": {"string": "value"} hoặc {"long": 123}}
        """
        extracted = {}
        
        for field_name, value_wrapper in row_data.items():
            if isinstance(value_wrapper, dict):
                # Extract value với type mapping
                for ogg_type, actual_value in value_wrapper.items():
                    mapped_value = self._map_ogg_type_to_python(ogg_type, actual_value)
                    extracted[field_name] = mapped_value
                    break
            else:
                extracted[field_name] = value_wrapper
                
        return extracted
    
    def _map_ogg_type_to_python(self, ogg_type: str, value: Any) -> Any:
        """
        Map OGG data types sang Python types cho PySpark DataFrame
        
        Args:
            ogg_type: OGG type từ JSON
            value: Giá trị gốc
            
        Returns:
            Giá trị đã được convert sang Python type phù hợp cho PySpark
        """
        if value is None or value == '':
            return None
            
        ogg_type_lower = ogg_type.lower()
        
        try:
            # STRING TYPES
            if ogg_type_lower in ['string', 'varchar', 'varchar2', 'char', 'nchar', 
                                'nvarchar', 'nvarchar2', 'text', 'longtext', 'mediumtext', 
                                'tinytext', 'clob', 'nclob', 'longvarchar', 'longnvarchar',
                                'uuid', 'guid', 'uniqueidentifier', 'xml', 'xmltype',
                                'geometry', 'geography', 'point', 'polygon', 'linestring',
                                'multipoint', 'multipolygon', 'multilinestring', 'geometrycollection',
                                'interval', 'interval year to month', 'interval day to second',
                                'rowid', 'urowid', 'bfile', 'ref', 'inet', 'cidr', 'macaddr',
                                'tsvector', 'tsquery', 'ntext', 'sql_variant', 'hierarchyid',
                                'year', 'enum', 'set']:
                return str(value) if value is not None else None
            
            # INTEGER TYPES  
            elif ogg_type_lower in ['int', 'integer', 'long', 'bigint', 'smallint', 
                                  'tinyint', 'mediumint', 'int2', 'int4', 'int8',
                                  'number', 'serial', 'bigserial', 'smallserial']:
                if isinstance(value, str) and '.' in value:
                    float_val = float(value)
                    if float_val.is_integer():
                        return int(float_val)
                    else:
                        return int(float_val)
                return int(value)
            
            # FLOAT/DECIMAL TYPES
            elif ogg_type_lower in ['float', 'double', 'real', 'decimal', 'numeric', 
                                  'money', 'smallmoney', 'float4', 'float8',
                                  'double precision', 'binary_float', 'binary_double']:
                return float(value)
            
            # BOOLEAN TYPES
            elif ogg_type_lower in ['boolean', 'bool', 'bit']:
                if isinstance(value, str):
                    return value.lower() in ('true', '1', 'yes', 'on', 'y', 't')
                elif isinstance(value, (int, float)):
                    return bool(value)
                return bool(value)
            
            # DATE/TIME TYPES - keep as string for PySpark processing
            elif ogg_type_lower in ['date', 'datetime', 'datetime2', 'smalldatetime',
                                  'timestamp', 'timestamp_ltz', 'timestamp_ntz', 
                                  'timestamptz', 'time', 'timetz']:
                return str(value) if value is not None else None
            
            # BINARY TYPES
            elif ogg_type_lower in ['binary', 'varbinary', 'longvarbinary', 'blob',
                                  'longblob', 'mediumblob', 'tinyblob', 'image',
                                  'raw', 'long raw', 'bytea']:
                if isinstance(value, str):
                    try:
                        return bytearray.fromhex(value.replace(' ', '').replace('\\x', ''))
                    except ValueError:
                        return value.encode('utf-8')
                return value
            
            # JSON TYPES - keep as string for PySpark JSON functions
            elif ogg_type_lower in ['json', 'jsonb']:
                return str(value) if value is not None else None
            
            # ARRAY TYPES - keep as string for PySpark array functions
            elif ogg_type_lower in ['array', 'varray']:
                return str(value) if value is not None else None
            
            # DEFAULT: treat as string
            else:
                return str(value) if value is not None else None
                
        except (ValueError, TypeError, OverflowError) as e:
            print(f"Warning: Cannot convert value '{value}' of type '{ogg_type}': {str(e)}. Using original value.")
            return value
        except Exception as e:
            print(f"Error: Unexpected error converting value '{value}' of type '{ogg_type}': {str(e)}. Using original value.")
            return value
    
    def _create_final_record(self, op_type: str, before: Dict, after: Dict) -> Dict[str, Any]:
        """Tạo final record dựa trên operation type"""
        if op_type == OGGOperationType.INSERT.value:
            return after.copy()
        
        elif op_type == OGGOperationType.UPDATE.value:
            # Merge before và after, ưu tiên after
            final_record = before.copy()
            final_record.update(after)
            return final_record
        
        elif op_type == OGGOperationType.DELETE.value:
            # Sử dụng before data và đánh dấu deleted
            final_record = before.copy()
            final_record['_is_deleted'] = True
            return final_record
        
        else:  # INITIAL hoặc unknown
            return after if after else before
    
    def _infer_spark_schema(self, sample_records: List[Dict[str, Any]]) -> StructType:
        """
        Infer PySpark schema từ sample records với OGG type mapping
        
        Args:
            sample_records: List các processed records
            
        Returns:
            StructType schema cho PySpark DataFrame
        """
        fields = []
        
        # Collect all field names và types
        all_fields = {}
        
        for record in sample_records:
            for field_name, value in record.items():
                if field_name not in all_fields:
                    all_fields[field_name] = self._infer_spark_type(value)
        
        # Create StructFields
        for field_name, spark_type in all_fields.items():
            fields.append(StructField(field_name, spark_type, True))
        
        return StructType(fields)
    
    def _infer_spark_type(self, value: Any):
        """Infer PySpark DataType từ Python value"""
        if value is None:
            return StringType()
        elif isinstance(value, bool):
            return BooleanType()
        elif isinstance(value, int):
            return LongType()
        elif isinstance(value, float):
            return DoubleType()
        elif isinstance(value, (bytes, bytearray)):
            return BinaryType()
        elif isinstance(value, list):
            return ArrayType(StringType())
        elif isinstance(value, dict):
            return MapType(StringType(), StringType())
        else:
            return StringType()
    
    def to_spark_dataframe(self, ogg_records: List[Union[str, Dict]]) -> DataFrame:
        """
        Chuyển đổi list OGG records thành PySpark DataFrame
        
        Args:
            ogg_records: List các OGG CDC records
            
        Returns:
            PySpark DataFrame với data đã được typed và processed
        """
        if not ogg_records:
            # Return empty DataFrame với basic schema
            schema = StructType([
                StructField("_ogg_table", StringType(), True),
                StructField("_ogg_operation", StringType(), True),
                StructField("_ogg_op_timestamp", StringType(), True),
                StructField("_ogg_current_timestamp", StringType(), True),
                StructField("_ogg_position", StringType(), True)
            ])
            return self.spark.createDataFrame([], schema)
        
        processed_records = []
        
        for record in ogg_records:
            parsed = self.parse_ogg_record(record)
            
            # Tạo flat record
            flat_record = parsed['final_record'].copy()
            
            # Thêm metadata
            flat_record.update({
                '_ogg_table': parsed['metadata']['table_name'],
                '_ogg_operation': parsed['metadata']['operation_type'],
                '_ogg_op_timestamp': parsed['metadata']['operation_timestamp'],
                '_ogg_current_timestamp': parsed['metadata']['current_timestamp'],
                '_ogg_position': parsed['metadata']['position']
            })
            
            processed_records.append(flat_record)
        
        # Infer schema
        schema = self._infer_spark_schema(processed_records)
        
        # Create DataFrame
        df = self.spark.createDataFrame(processed_records, schema)
        
        # Apply type optimizations
        df = self._optimize_spark_dataframe_types(df)
        
        return df
    
    def _optimize_spark_dataframe_types(self, df: DataFrame) -> DataFrame:
        """
        Optimize PySpark DataFrame types dựa trên data patterns
        
        Args:
            df: Raw PySpark DataFrame
            
        Returns:
            DataFrame với optimized types
        """
        optimized_df = df
        
        # Convert datetime strings to timestamp types
        datetime_patterns = ['DATE', 'TIME', 'TS', '_TIMESTAMP']
        
        for col_name in df.columns:
            if any(pattern in col_name.upper() for pattern in datetime_patterns):
                try:
                    # Try to convert to timestamp
                    optimized_df = optimized_df.withColumn(
                        col_name,
                        to_timestamp(col(col_name))
                    )
                except:
                    # If fails, keep as string
                    pass
        
        return optimized_df
    
    def prepare_for_hudi(self, df: DataFrame) -> DataFrame:
        """
        Chuẩn bị PySpark DataFrame cho Apache Hudi
        
        Args:
            df: DataFrame từ to_spark_dataframe()
            
        Returns:
            DataFrame sẵn sàng cho Hudi upsert
        """
        if df.count() == 0:
            return df
        
        hudi_df = df
        
        # Thêm Hudi metadata columns
        hudi_df = hudi_df.withColumn(
            "_hoodie_record_key", 
            col(self.record_key_field).cast(StringType())
        )
        
        # Partition path
        if self.partition_key_field and self.partition_key_field in df.columns:
            hudi_df = hudi_df.withColumn(
                "_hoodie_partition_path",
                regexp_replace(
                    concat(lit(f"{self.partition_key_field.lower()}="), 
                           col(self.partition_key_field).cast(StringType())),
                    "[^a-zA-Z0-9=_-]", "_"
                )
            )
        else:
            hudi_df = hudi_df.withColumn("_hoodie_partition_path", lit("default"))
        
        # Commit time
        hudi_df = hudi_df.withColumn(
            "_hoodie_commit_time",
            regexp_replace(
                date_format(current_timestamp(), "yyyyMMddHHmmssSSS"),
                "[-: ]", ""
            )
        )
        
        # Precombine field (sử dụng OGG timestamp)
        if '_ogg_op_timestamp' in df.columns:
            hudi_df = hudi_df.withColumn(
                "_hoodie_commit_seqno",
                unix_timestamp(to_timestamp(col('_ogg_op_timestamp')))
            )
        else:
            hudi_df = hudi_df.withColumn(
                "_hoodie_commit_seqno",
                unix_timestamp()
            )
        
        return hudi_df
    
    def prepare_for_delta(self, df: DataFrame) -> DataFrame:
        """
        Chuẩn bị PySpark DataFrame cho Delta Lake
        
        Args:
            df: DataFrame từ to_spark_dataframe()
            
        Returns:
            DataFrame sẵn sàng cho Delta merge
        """
        if df.count() == 0:
            return df
        
        delta_df = df
        
        # Thêm Delta metadata
        delta_df = delta_df.withColumn(
            "_delta_operation",
            when(col("_ogg_operation") == "I", lit("INSERT"))
            .when(col("_ogg_operation") == "U", lit("UPDATE"))
            .when(col("_ogg_operation") == "D", lit("DELETE"))
            .otherwise(lit("UPSERT"))
        )
        
        # Convert timestamps
        timestamp_cols = ['_ogg_op_timestamp', '_ogg_current_timestamp']
        for col_name in timestamp_cols:
            if col_name in df.columns:
                delta_df = delta_df.withColumn(
                    col_name,
                    to_timestamp(col(col_name))
                )
        
        return delta_df
    
    def get_table_schema_spark(self, sample_records: List[Union[str, Dict]]) -> Dict[str, Dict[str, str]]:
        """
        Phân tích schema từ sample records với OGG type mapping cho PySpark
        
        Args:
            sample_records: Sample OGG records để phân tích schema
            
        Returns:
            Dictionary với structure:
            {
                'field_name': {
                    'ogg_type': 'string',
                    'python_type': 'str',
                    'spark_type': 'StringType'
                }
            }
        """
        if not sample_records:
            return {}
        
        schema = {}
        
        # Collect tất cả field types từ sample records
        for record in sample_records:
            parsed = self.parse_ogg_record(record)
            
            # Get raw row data để extract original types
            if isinstance(record, str):
                record_dict = json.loads(record)
            else:
                record_dict = record
                
            for section in ['before', 'after']:
                row_data = record_dict.get(section, {}).get('row', {})
                for field_name, value_wrapper in row_data.items():
                    if isinstance(value_wrapper, dict):
                        for ogg_type, actual_value in value_wrapper.items():
                            if field_name not in schema:
                                python_type = self._get_python_type_name(ogg_type, actual_value)
                                spark_type = self._get_spark_type_name(ogg_type, actual_value)
                                
                                schema[field_name] = {
                                    'ogg_type': ogg_type,
                                    'python_type': python_type,
                                    'spark_type': spark_type,
                                    'sample_value': actual_value
                                }
                            break
        
        return schema
    
    def _get_python_type_name(self, ogg_type: str, value: Any) -> str:
        """Get Python type name cho schema documentation"""
        ogg_type_lower = ogg_type.lower()
        
        if ogg_type_lower in ['string', 'varchar', 'varchar2', 'char', 'nchar', 
                            'nvarchar', 'nvarchar2', 'text', 'longtext', 'mediumtext', 
                            'tinytext', 'clob', 'nclob', 'longvarchar', 'longnvarchar',
                            'uuid', 'guid', 'uniqueidentifier', 'xml', 'xmltype',
                            'geometry', 'geography', 'point', 'polygon', 'linestring',
                            'multipoint', 'multipolygon', 'multilinestring', 'geometrycollection',
                            'interval', 'interval year to month', 'interval day to second',
                            'rowid', 'urowid', 'bfile', 'ref', 'inet', 'cidr', 'macaddr',
                            'tsvector', 'tsquery', 'ntext', 'sql_variant', 'hierarchyid',
                            'year', 'enum', 'set', 'json', 'jsonb', 'array', 'varray']:
            return 'str'
        elif ogg_type_lower in ['int', 'integer', 'long', 'bigint', 'smallint', 
                              'tinyint', 'mediumint', 'int2', 'int4', 'int8',
                              'number', 'serial', 'bigserial', 'smallserial']:
            return 'int'
        elif ogg_type_lower in ['float', 'double', 'real', 'decimal', 'numeric', 
                              'money', 'smallmoney', 'float4', 'float8',
                              'double precision', 'binary_float', 'binary_double']:
            return 'float'
        elif ogg_type_lower in ['boolean', 'bool', 'bit']:
            return 'bool'
        elif ogg_type_lower in ['date', 'datetime', 'datetime2', 'smalldatetime',
                              'timestamp', 'timestamp_ltz', 'timestamp_ntz', 
                              'timestamptz', 'time', 'timetz']:
            return 'datetime'
        elif ogg_type_lower in ['binary', 'varbinary', 'longvarbinary', 'blob',
                              'longblob', 'mediumblob', 'tinyblob', 'image',
                              'raw', 'long raw', 'bytea']:
            return 'bytes'
        else:
            return 'str'
    
    def _get_spark_type_name(self, ogg_type: str, value: Any) -> str:
        """Get PySpark type name cho schema documentation"""
        ogg_type_lower = ogg_type.lower()
        
        if ogg_type_lower in ['string', 'varchar', 'varchar2', 'char', 'nchar', 
                            'nvarchar', 'nvarchar2', 'text', 'longtext', 'mediumtext', 
                            'tinytext', 'clob', 'nclob', 'longvarchar', 'longnvarchar',
                            'uuid', 'guid', 'uniqueidentifier', 'xml', 'xmltype',
                            'geometry', 'geography', 'point', 'polygon', 'linestring',
                            'multipoint', 'multipolygon', 'multilinestring', 'geometrycollection',
                            'interval', 'interval year to month', 'interval day to second',
                            'rowid', 'urowid', 'bfile', 'ref', 'inet', 'cidr', 'macaddr',
                            'tsvector', 'tsquery', 'ntext', 'sql_variant', 'hierarchyid',
                            'year', 'enum', 'set']:
            return 'StringType'
        elif ogg_type_lower in ['int', 'integer', 'smallint', 'tinyint', 'mediumint']:
            return 'IntegerType'
        elif ogg_type_lower in ['long', 'bigint', 'int8', 'number', 'serial', 'bigserial']:
            return 'LongType'
        elif ogg_type_lower in ['float', 'real', 'float4']:
            return 'FloatType'
        elif ogg_type_lower in ['double', 'double precision', 'float8', 'binary_double']:
            return 'DoubleType'
        elif ogg_type_lower in ['decimal', 'numeric', 'money', 'smallmoney']:
            return 'DecimalType'
        elif ogg_type_lower in ['boolean', 'bool', 'bit']:
            return 'BooleanType'
        elif ogg_type_lower in ['date']:
            return 'DateType'
        elif ogg_type_lower in ['datetime', 'datetime2', 'smalldatetime',
                              'timestamp', 'timestamp_ltz', 'timestamp_ntz', 
                              'timestamptz']:
            return 'TimestampType'
        elif ogg_type_lower in ['binary', 'varbinary', 'longvarbinary', 'blob',
                              'longblob', 'mediumblob', 'tinyblob', 'image',
                              'raw', 'long raw', 'bytea']:
            return 'BinaryType'
        elif ogg_type_lower in ['json', 'jsonb']:
            return 'StringType'  # Will be parsed with from_json later
        elif ogg_type_lower in ['array', 'varray']:
            return 'ArrayType(StringType)'
        else:
            return 'StringType'

# Utility functions cho PySpark integration
def create_hudi_upsert_config(table_name: str, 
                            record_key: str = "_hoodie_record_key",
                            partition_path: str = "_hoodie_partition_path",
                            precombine_field: str = "_hoodie_commit_seqno") -> Dict[str, str]:
    """
    Tạo Hudi configuration cho upsert với PySpark
    
    Args:
        table_name: Tên Hudi table
        record_key: Record key field
        partition_path: Partition path field
        precombine_field: Precombine field
        
    Returns:
        Hudi configuration dict
    """
    return {
        'hoodie.table.name': table_name,
        'hoodie.datasource.write.recordkey.field': record_key,
        'hoodie.datasource.write.partitionpath.field': partition_path,
        'hoodie.datasource.write.table.name': table_name,
        'hoodie.datasource.write.operation': 'upsert',
        'hoodie.datasource.write.precombine.field': precombine_field,
        'hoodie.upsert.shuffle.parallelism': '2',
        'hoodie.insert.shuffle.parallelism': '2',
        'hoodie.datasource.write.hive_style_partitioning': 'true',
        'hoodie.datasource.write.table.type': 'COPY_ON_WRITE'
    }

# Example usage
if __name__ == "__main__":
    from pyspark.sql.functions import date_format, concat, unix_timestamp
    
    # Tạo Spark session
    spark = SparkSession.builder \
        .appName("OGGCDCProcessor") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()
    
    # Sample OGG CDC data với comprehensive types
    sample_ogg_record = {
        "table": {"string": "LOGMINER.COMPREHENSIVE_TABLE"},
        "op_type": {"string": "U"},
        "op_ts": {"string": "2025-08-27 13:53:01.035610"},
        "current_ts": {"string": "2025-08-27 13:54:15.887000"},
        "pos": {"string": "00000000000000029346"},
        "before": {
            "row": {
                "CUSTOMER_ID": {"string": "33d48cdc-0f51-4fcd-a7aa-28711fdb2442"},
                "ACCOUNT_NUMBER": {"varchar": "ACC454696"},
                "RISK_SCORE": {"long": 10},
                "CREDIT_LIMIT": {"double": 50000.75},
                "IS_ACTIVE": {"boolean": True},
                "CREATE_DATE": {"date": "2023-01-15"},
                "LAST_UPDATE": {"timestamp": "2025-08-27 13:53:01.035610"},
                "JSON_DATA": {"json": '{"key": "value"}'},
                "BATCH_ID": {"string": "f1256990-60f0-4621-9f84-0ffef385a1c6"},
                "NOTETT": {"string": "ABCD"}
            }
        },
        "after": {
            "row": {
                "CUSTOMER_ID": {"string": "33d48cdc-0f51-4fcd-a7aa-28711fdb2442"},
                "RISK_SCORE": {"long": 15},
                "CREDIT_LIMIT": {"double": 75000.50},
                "IS_ACTIVE": {"boolean": False},
                "NOTETT": {"string": "ABCDE"}
            }
        }
    }
    
    # Khởi tạo processor
    processor = OGGCDCProcessor(
        spark_session=spark,
        record_key_field="CUSTOMER_ID",
        partition_key_field="BATCH_ID"
    )
    
    # Convert to PySpark DataFrame
    df = processor.to_spark_dataframe([sample_ogg_record])
    
    print("=== PYSPARK DATAFRAME INFO ===")
    print(f"Row count: {df.count()}")
    print(f"Column count: {len(df.columns)}")
    
    print("\n=== SCHEMA ===")
    df.printSchema()
    
    print("\n=== SAMPLE DATA ===")
    df.select("CUSTOMER_ID", "RISK_SCORE", "CREDIT_LIMIT", "IS_ACTIVE", "_ogg_operation").show(truncate=False)
    
    # Schema analysis
    schema = processor.get_table_schema_spark([sample_ogg_record])
    print(f"\n=== SCHEMA ANALYSIS ===")
    for field, type_info in schema.items():
        print(f"{field:<20}: {type_info['ogg_type']:<10} → {type_info['spark_type']}")
    
    # Prepare for Hudi
    hudi_df = processor.prepare_for_hudi(df)
    print(f"\n=== HUDI-READY DATAFRAME ===")
    print("Hudi columns:")
    hudi_cols = [col for col in hudi_df.columns if col.startswith('_hoodie_')]
    hudi_df.select(*hudi